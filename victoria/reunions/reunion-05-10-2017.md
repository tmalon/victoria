Compte-rendu réunion du 05 octobre 2017
===================


La réunion n°1 a eu lieu **jeudi 05 octobre 2017 de 11h30 à 12h30** en présence de **Sylvie**. Un fichier de suivi de thèse est mis en place sur **Overleaf**[^overleaf]. 

Les anotations suivantes seront utilisées :

<i class="icon-left"></i> Indique un retour de l'équipe d'encadrement.

<i class="icon-attention"></i> Indique une tâche à faire pour l'équipe d'encadrement.

<i class="icon-right-hand"></i> Indique une tâche à faire pour le doctorant.

L'ensemble des tâches à faire est récapitulé à la fin du compte-rendu.


[TOC]

----------
Ordres du jour
-------------
À l'ordre du jour de cette première réunion :

> **Ont été présentés :**

> - L'état de l'avancée des inscriptions administratives.
> - Les premières analyses faites sur les vidéos filmées à l'UPS.
> - Les premières investigations par rapport au code d'Axel.

> **Partie questions/réponses/entretien**

> - L'état de l'avancée des inscriptions administratives.
> - Les premières analyses faites sur les vidéos filmées à l'UPS.
> - Les premières investigations par rapport au code d'Axel.

#### <i class="icon-circle"></i> Divers

- Installation de Matlab

- Travail temporairement avec ordinateur personnel : problème de câble (uniquement sortie HDMI), incompatible avec les écrans disponibles de la F214 qui prennent VGA (bleu) ou DVI (blanc), ou avec les rétroprojecteurs de la plusieurs des salles de réunion de l'ENSEEIHT qui sont en VGA. **Il faudrait un adaptateur HDMI/VGA**. 

<i class="icon-right-hand"></i> Voir avec le serveur informatique pour demander s'ils ont un adaptateur de câble ou le commander.

#### <i class="icon-file"></i> Inscriptions administratives

- Un compte a été créé pour se connecter au réseau de l'école. Les adresse thierry.malon@enseeiht.fr et thierry.malon@irit.fr sont créées et fonctionnelles.

- Les documents d'inscription à l'INP ont été retournés jeudi 28 septembre 2017. Pas de réponse pour le moment. Il fautdra peut-être leur passer un coup de fil. 

- L'inscription auprès de l'UPS en vue d'obtenir le contrat de travail doit être refaite. En effet, Arnaude CARIOU a envoyé un mail précisant que *"Le dossier de recrutement que vous avez rempli n'est pas le bon car au début, il avait été convenu que vous soyez recruté en ingénieur d'études"*, avec un nouveau dossier à remplir. De nouvelles pièces sont demandées, notamment une **copie du certificat de scolarité de l’année universitaire au titre de laquelle est demandé le contrat** qui sera délivrée par l'INP.

<i class="icon-right-hand"></i> Appeler l'INP en précisant que c'est bloquant.

- Une entrevue avec Géraldine aura lieu prochainement (soit aujourd'hui même, soit en début de semaine prochaine) afin de définir la charge de DCCE. Géraldine a proposé que le tuteur soit un enseignant d'un parcours différent (réseau ou mathématique) afin d'avoir un encadrement diversifié. Au niveau des UE, j'ai proposé d'avoir au moins un groupe de TP sur Java (équivalent d'au moins une dizaine de séances), ainsi qu'une UE de maths et une UE de réseau (à définir).

<i class="icon-right-hand"></i> Essayer de croiser Géraldine.

#### <i class="icon-hdd"></i> Vidéos du dataset

Après téléchargement sur le Google Drive des différentes vidéos filmées en juillet à l'UPS et analyse de ces dernières, il apparaît :

- Une forte diversité en terme de formats (.MOV, .MTS, .mp4), en terme de résolutions (1920 x 1080, 640 x 480, 1280 x 720, 1440 x 1080, 1920 x 1088), en terme de framerate (25 fps, 26 fps, 28 fps, 29 fps, 30 fps, 50 fps), en terme de tailles de fichiers (pour le scénario 1, de 150Mo à 1,7Go). Certaines vidéos ne sont pas complètes (manque de quelques secondes au début ou coupure(s) en cours de vidéo). Les détails précis sont répertoriés dans un tableau sur un document **Overleaf**[^overleaf].

<i class="icon-left"></i> Diversité faite exprès pour correspondre à des conditions réelles.

- Des vidéos non synchronisées (ce qui est normal).

- Des vidéos lourdes en terme de capacité mémoire.

- Quelques vidéos manquantes.

<i class="icon-attention"></i> Pour la caméra 2 (films 1 et 2), les vidéos faisaient 0 octet car le téléchargement avait été interrompu avant la fin. Les vraies vidéos sont toujours disponibles et seront mises sur Google Drive bientôt.

<i class="icon-attention"></i> Deux petites erreurs à corriger : pour le film 1 caméra 2 (Bérangère) qui était filmé en accéléré, il a été mis sur Google Drive la même vidéo que pour le film 1 caméra 1, le mieux serait de le retirer et de ne rien mettre, ou de mettre celui en accéléré. Pour le film 3 caméra 1 (Vincent A) où il manque une minute, il a été mis sur Google Drive la même vidéo que pour le film 3 caméra 2.

<i class="icon-left"></i> Certaines vidéos ne nous seront pas utiles (éloignées) et servent à une autre équipe. Il s'agit notamment des vidéos 8 à 13, donc ce n'est pas grave qu'il manque les vidéos 8 et 13.

Travail effectué :

- Recherche des correspondances entre numéros des vidéos et numéros des photos des vues distribuées lors du tournage.

- Détection précise à la frame près du timing des cornes de brume sur les vidéos qui ont un framerate standard (25 fps, 30 fps, 50 fps). Pour certaines vidéos du **scénario 1**, après utilisation du logiciel de traitement vidéo **Kdenlive**[^kdenlive], ont été retirées la partie qui précède le début du son de la première corne de brume et la partie qui suit le début du son de la deuxième corne de  brume. Elles sont ainsi *à peu près synchronisées* (car fait à la main et à partir de la détection d'un son, ce qui est discutable), disons pseudo-synchronisées toutes d'une durée de 4 min 48 sec.

<i class="icon-right-hand"></i> Demander à Julien Pinquier s'ils n'ont pas dans l'équipe SAMOVA des outils permettant de synchroniser, et des outils permettant d'afficher de multiples sources vidéos en simultané.

- Utilisation de avconv pour générer à partir de ces vidéos pseudo-synchronisées des séries d'images à un rythme régulier. Par exemple, l'utilisation de la commande...

`avconv -i F1C1.mp4 -r 25 -f image2 ./imgF1C1/image_%4d.jpeg`

- ...génère à partir de la vidéo F1C1.mp4 (pour Film 1 Caméra 1) un ensemble d'image (25 par seconde de vidéo) dans le dossier spécifié avec l'extension .jpeg et des titres d'image sur 4 digits (0001.jpeg, 0002.jpeg, 0003.jpeg...). Pour le moment, utilisation de cette commande avec 1 image par seconde, produisant 290 images (alors que les vidéos durent 288 secondes).

- Pour les autres vidéos, la détection n'a eu lieu qu'à la seconde près, en attendant de décider ensemble comment traîter la diversité des framerates.

- Évaluation de la pire erreur temporelle (en seconde) entre deux vidéos $V_1$ et $V_2$ ayant des framerates respectifs $f_1$ > $f_2$. Après calculs, elle est atteinte et vaut de $\frac{PartieEntière(\frac{\min(f_1,f_2)}{2})}{f_1*f_2}$.  Plus simplement, elle vaut (sans être toujours atteinte) $\frac{1}{2 f_2}$. Ainsi pour deux vidéos à respectivements 27fps et 25fps, le pire écart temporel entre images issues de chaque vidéo est de 20 ms. Cela paraît relativement faible.

<i class="icon-left"></i> OK.

#### <i class="icon-folder-open"></i> Code d'Axel

- Lecture du fichier README détaillant les différentes étapes à effectuer :

`This document describes the succession of steps to complete in order to go from a set of images to the video query system.`

`1) Assign a number N to the set of images (i.e. sequence) you want to work on, and launch createSequence(N) in the data/ directory.`

`If you have an algo to calibrate the cameras then jump straight to step 4 `

`2) In order to click the features, run :
	- main_featuresClicker(N) (in FeaturesClicker)
		- first click the points on the basis images (0)
		- then on the resection images               (1)
	- createFiles(N)
The createFiles module will produce the featuresZhang.txt and qbc.mat files`

`3) Apply SFM and resection codes to obtain the cameras information :
	-> N-ba-resection-reconstr-vincent-cp2.mat`

`4) Produce the masks for PMVS, thanks to the cones intersection.
	- To sample points in the cones intersection:
		generateCones3DPoints(N)
	- To produce cones saliency maps, and cones binary masks
		generateConesSaliency(N)`

`5) Run PMVS, level 3`

`6) Perform some 3D clustering, and produce saliency maps and requests data
	- produceInterestMaps(N,plotFlag,queryData)`

`7) query_tester(N) to test the 3D query
	- query_launcher : TO DO`

- Modifications et appropriation du code pour les étapes 1 et 2 (création des dossiers et clics de points features). Création d'un début de programme principal qui fait les appels aux différentes fonctions nécessaires.

- L'étape 3 était effectuée par un code de Lilian, non retrouvé pour le moment.

- Après test avec Axel sur les photos de la fanfare, l'étape 4 est fonctionnelle (génération des cônes et de leur intersection).

- Pour l'étape 5, pmvs a été installé. Cependant, il ne s'agit pas de la même version que celle utilisée à l'époque et développée par Yasutaka Furukawa et Jean Ponce. Celle-ci a été reprise par Pierre Moulon. De plus, aucune documentation n'était disponible (tous les liens étaient morts) : test de nouveau ce matin, il semblerait que la documentation soit de nouveau en ligne (https://www.di.ens.fr/pmvs/documentation.html).

<i class="icon-right-hand"></i> Continuer à s'approprier le code d'Axel ainsi que PMVS.

#### <i class="icon-file"></i> Lectures de papier

Lecture de P. Shrestha, M. Barbieri, H. Weda, and D. Sekulovski, “Synchronization of multiple camera videos using audio-visual features,” IEEE Transactions on Multimedia, vol. 12, no. 1, pp. 79–92, 2010

- Papiers cités dans les références de l'article d'Axel.
- Date un peu, voir si plus récent sur le sujet.
- Traite du problème d'alternances de vue pour rendre "une" vidéo (en vérité plusieurs) moins ennuyante en faisant varier les points de vue, premier problème soulevé celui de la synchronisation (appelé time-alignment).
- Définit la synchronisation audio-video : au delà d'un écart de +45ms -125ms entre audio et vidéo (valeur positive indique que l'audio est en avance sur la vidéo) on ressent l'écart (notamment au niveau des mouvements de lèvres).
- Mais ils utilisent un flash pour calibrer (instant/frame avec une luminosité plus marquée), ce que l'on a pas.

<i class="icon-right-hand"></i> Chercher d'autres articles et commencer de la bibliographie.

<i class="icon-attention"></i> Donner quelques pistes d'articles à lire.

Questions / réponses / entretien
-------------
- Partie calibrage :
Pour le moment on reprend le code d'Axel et on clique manuellement des points features pour le calibrage. Puis on voudra sans doute automatiser le calibrage : ***vers quelles solutions se tourner ?***

<i class="icon-left"></i> A voir peut-être plus tard, pour le moment faire marcher le code d'Axel pour avoir une base.

- Bien définir ce qui est attendu :
pour Axel les cônes s'intersectent naturellement au niveau des régions d'intérêt pour faciliter les transitions d'une région à une autre. ***A-t-on besoin des notions de régions d'intérêt dans notre cas ? Des actions peuvent avoir lieu partout sur la scène (personnage qui marche d'un point A à un point B en traversant tout l'écran).***

<i class="icon-left"></i> En effet, il faudrait définir avec Vincent ce qui est vraiment attendu, ce qu'il avait en tête. Dans le livrable dont les intitulés sont vagues.

- Synchronisation :
faite à la man en se fiant au son de la corne de brume. Propagation du son à 340 m/s (pour un framerate de 25fps cela fait 1 frame = 13,4 mètres : les caméras étant distantes les unes des autres à des distances de plusieurs dizaines de mètres, cela peut engendrer des différences de plusieurs frames. D'autant plus que la détection s'est faite à l'oreille humaine. ***Quelle importance joue la précision ? Comment peut-on automatiser ?***

<i class="icon-left"></i> Comme dit plus haut, à voir avec Julien de l'équipe SAMOVA.

- A l'arrivée, on voudrait une scène en 3D navigable produite à partir des différentes vidéos.
***Qu'en est-il du son ?***

<i class="icon-left"></i> Bonne question, on n'est pas vraiment concernés car on n'a pas prévu de s'occuper du son, mais il faudrait demander à l'équipe SAMOVA lors d'une réunion ou autre comment ils comptent s'y prendre.

Récapitulatif des choses à faire pour la prochaine réunion
-------------

<i class="icon-attention"></i> Pour la caméra 2 (films 1 et 2), les vidéos faisaient 0 octet car le téléchargement avait été interrompu avant la fin. Les vraies vidéos sont toujours disponibles et seront mises sur Google Drive bientôt.

<i class="icon-attention"></i> Deux petites erreurs à corriger : pour le film 1 caméra 2 (Bérangère) qui était filmé en accéléré, il a été mis sur Google Drive la même vidéo que pour le film 1 caméra 1, le mieux serait de le retirer et de ne rien mettre, ou de mettre celui en accéléré. Pour le film 3 caméra 1 (Vincent A) où il manque une minute, il a été mis sur Google Drive la même vidéo que pour le film 3 caméra 2.

<i class="icon-attention"></i> Donner quelques pistes d'articles à lire.

<i class="icon-right-hand"></i> Voir avec le serveur informatique pour demander s'ils ont un adaptateur de câble ou le commander : **FAIT** ils n'ont pas

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i class="icon-right-hand"></i> Leur envoyer un lien vers un site vendant l'adaptateur souhaité après avoir défini sur quel budget il serait payé.

<i class="icon-right-hand"></i> Appeler l'INP en précisant que c'est bloquant : **FAIT** ("Votre dossier est sur mon bureau") et rendez-vous pris mercredi pour le paiement.

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<i class="icon-right-hand"></i> Se rendre à l'INP mercredi 11 octobre 2017 entre 9h et 11h30.

<i class="icon-right-hand"></i> Essayer de croiser Géraldine.

<i class="icon-right-hand"></i> Demander à Julien Pinquier s'ils n'ont pas dans l'équipe SAMOVA des outils permettant de synchroniser, et des outils permettant d'afficher de multiples sources vidéos en simultané.

<i class="icon-right-hand"></i> Continuer à s'approprier le code d'Axel ainsi que PMVS.

<i class="icon-right-hand"></i> Chercher d'autres articles et commencer de la bibliographie.

Fin de la réunion
-------------

Prochaine réunion vendredi 13 octobre à 9 heures à l'ENSEEIHT.

  [^footnote]: Note de bas de page **vous êtes arrivés à la fin du document**.

  
  
  [^overleaf]: [Lien OverLeaf](https://www.overleaf.com/read/xqxhsnnzwhxs) vers le fichier de suivi de thèse.
  

  
  [^kdenlive]: [Lien vers la documentation Ubuntu](https://doc.ubuntu-fr.org/kdenlive) vers le fichier de suivi de thèse.
